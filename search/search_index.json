{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to Kubernetes Fundamentals by School of Devops\n\n\nThis is a Lab Guide which goes along with the Docker and Kubernetes course by School of Devops.\n\n\nFor information about the devops trainign courses visit \nschoolofdevops.com\n.\n\n\nTeam\n\n\n\n\nGourav Shah\n\n\nVijayboopathy\n\n\nVenkat", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-kubernetes-fundamentals-by-school-of-devops", 
            "text": "This is a Lab Guide which goes along with the Docker and Kubernetes course by School of Devops.  For information about the devops trainign courses visit  schoolofdevops.com .", 
            "title": "Welcome to Kubernetes Fundamentals by School of Devops"
        }, 
        {
            "location": "/#team", 
            "text": "Gourav Shah  Vijayboopathy  Venkat", 
            "title": "Team"
        }, 
        {
            "location": "/kube-cluster-vagrant/", 
            "text": "Install VirtualBox and Vagrant\n\n\n\n\n\n\n\n\nTOOL\n\n\nVERSION\n\n\nLINK\n\n\n\n\n\n\n\n\n\n\nVirtualBox\n\n\n5.1.26\n\n\nhttps://www.virtualbox.org/wiki/Downloads\n\n\n\n\n\n\nVagrant\n\n\n1.9.7\n\n\nhttps://www.vagrantup.com/downloads.html\n\n\n\n\n\n\n\n\nImporting a  VM Template\n\n\nvagrant box list\n\nvagrant box add ubuntu/xenial64 ubuntu-xenial64.box\n\nvagrant box list\n\n\n\n\n\nProvisioning Vagrant Nodes\n\n\nClone repo if not already\n\n\ngit clone https://github.com/schoolofdevops/lab-setup.git\n\n\n\n\n\n\nLaunch environments with Vagrant\n\n\ncd lab-setup/kubernetes/vagrant\n\nvagrant up\n\n\n\n\n\nLogin to nodes\n\n\nOpen three different terminals to login to 3 nodes created with above command\n\n\nTerminal 1\n\n\nvagrant ssh kube-01\nsudo su\n\n\n\n\n\nTerminal 2\n\n\nvagrant ssh kube-02\nsudo su\n\n\n\n\nTerminal 3\n\n\nvagrant ssh kube-03\nsudo su\n\n\n\n\nOnce the environment is setup, follow \nInitialization of Master\n onwards from the following tutorial\nhttps://github.com/schoolofdevops/kubernetes-fundamentals/blob/master/tutorials/1.%20install_kubernetes.md", 
            "title": "Provisioning VMs with Vagrant"
        }, 
        {
            "location": "/kube-cluster-vagrant/#install-virtualbox-and-vagrant", 
            "text": "TOOL  VERSION  LINK      VirtualBox  5.1.26  https://www.virtualbox.org/wiki/Downloads    Vagrant  1.9.7  https://www.vagrantup.com/downloads.html", 
            "title": "Install VirtualBox and Vagrant"
        }, 
        {
            "location": "/kube-cluster-vagrant/#importing-a-vm-template", 
            "text": "vagrant box list\n\nvagrant box add ubuntu/xenial64 ubuntu-xenial64.box\n\nvagrant box list", 
            "title": "Importing a  VM Template"
        }, 
        {
            "location": "/kube-cluster-vagrant/#provisioning-vagrant-nodes", 
            "text": "Clone repo if not already  git clone https://github.com/schoolofdevops/lab-setup.git  Launch environments with Vagrant  cd lab-setup/kubernetes/vagrant\n\nvagrant up  Login to nodes  Open three different terminals to login to 3 nodes created with above command  Terminal 1  vagrant ssh kube-01\nsudo su  Terminal 2  vagrant ssh kube-02\nsudo su  Terminal 3  vagrant ssh kube-03\nsudo su  Once the environment is setup, follow  Initialization of Master  onwards from the following tutorial\nhttps://github.com/schoolofdevops/kubernetes-fundamentals/blob/master/tutorials/1.%20install_kubernetes.md", 
            "title": "Provisioning Vagrant Nodes"
        }, 
        {
            "location": "/1. install_kubernetes/", 
            "text": "Compatibility\n\n\nKubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\n\n\nThe below steps are applicabe for the below mentioned OS\n\n\n\n\n\n\n\n\nOS\n\n\nVersion\n\n\nCodename\n\n\n\n\n\n\n\n\n\n\nUbuntu\n\n\n16.04\n\n\nXenial\n\n\n\n\n\n\n\n\nBase Setup\n\n\n Skip this step and scroll to Initializing Master if you have setup nodes with vagrant\n\n\nOn all nodes which would be part of this cluster, you need to do the base setup as described here,\n\n\nCreate Kubernetes Repository\n\n\nWe need to create a repository to download Kubernetes.\n\n\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n\n\n\n\ncat \nEOF \n /etc/apt/sources.list.d/kubernetes.list\ndeb http://apt.kubernetes.io/ kubernetes-xenial main\nEOF\n\n\n\n\nInstallation of the packages\n\n\nWe should update the machines before installing so that we can update the repository.\n\n\napt-get update -y\n\n\n\n\nInstalling all the packages with dependencies:\n\n\napt-get install -y docker.io kubelet kubeadm kubectl kubernetes-cni\n\n\n\n\nrm -rf /var/lib/kubelet/*\n\n\n\n\nSetup sysctl configs\n\n\nIn order for many container networks to work, the following needs to be enabled on each node.\n\n\nsysctl net.bridge.bridge-nf-call-iptables=1\n\n\n\n\nThe above steps has to be followed in all the nodes.\n\n\nInitializing Master\n\n\nThis tutorial assumes \nkube-01\n  as the master and used kubeadm as a tool to install and setup the cluster. This section also assumes that you are using vagrant based setup provided along with this tutorial. If not, please update the IP address of the master accordingly.\n\n\nTo initialize master, run this on kube-01\n\n\nkubeadm init --apiserver-advertise-address 192.168.12.10 --pod-network-cidr=192.168.0.0/16\n\n\n\n\n\nInitialization of the Nodes (Previously Minions)\n\n\nAfter master being initialized, it should display the command which could be used on all worker/nodes to join the k8s cluster.\n\n\ne.g.\n\n\nkubeadm join --token c04797.8db60f6b2c0dd078 192.168.12.10:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0\n\n\n\n\nCopy and paste it on all node.\n\n\nTroubleshooting Tip\n\n\nIf you lose  the join token, you could retrieve it using\n\n\nkubeadm token list\n\n\n\n\nOn successfully joining the master, you should see output similar to following,\n\n\nroot@kube-03:~# kubeadm join --token c04797.8db60f6b2c0dd078 159.203.170.84:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0\n[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.\n[preflight] Running pre-flight checks\n[discovery] Trying to connect to API Server \n159.203.170.84:6443\n\n[discovery] Created cluster-info discovery client, requesting info from \nhttps://159.203.170.84:6443\n\n[discovery] Requesting info from \nhttps://159.203.170.84:6443\n again to validate TLS against the pinned public key\n[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server \n159.203.170.84:6443\n\n[discovery] Successfully established connection with API Server \n159.203.170.84:6443\n\n[bootstrap] Detected server version: v1.8.2\n[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)\n\nNode join complete:\n* Certificate signing request sent to master and response\n  received.\n* Kubelet informed of new secure connection details.\n\nRun 'kubectl get nodes' on the master to see this machine join.\n\n\n\n\nSetup the admin client - Kubectl\n\n\nOn Master Node\n\n\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n\n\n\nInstalling CNI with Weave\n\n\nInstalling overlay network is necessary for the pods to communicate with each other across the hosts. It is necessary to do this before you try to deploy any applications to your cluster.\n\n\nThere are various overlay networking drivers available for kubernetes. We are going to use \nWeave Net\n.\n\n\n\nexport kubever=$(kubectl version | base64 | tr -d '\\n')\nkubectl apply -f \nhttps://cloud.weave.works/k8s/net?k8s-version=$kubever\n\n\n\n\n\nValidating the Setup\n\n\nYou could validate the status of this cluster, health of pods and whether all the components are up or not by using a few or all of the following commands.\n\n\nTo check if nodes are ready\n\n\nkubectl get nodes\n\n\n\n\n\n[ Expected output ]\n\n\nroot@kube-01:~# kubectl get nodes\nNAME      STATUS    ROLES     AGE       VERSION\nkube-01   Ready     master    9m        v1.8.2\nkube-02   Ready     \nnone\n    4m        v1.8.2\nkube-03   Ready     \nnone\n    4m        v1.8.2\n\n\n\n\nAdditional Status Commands\n\n\nkubectl cluster-info\n\nkubectl get pods -n kube-system\n\nkubectl get events\n\n\n\n\n\nIt will take a few minutes to have the cluster up and running with all the services.\n\n\nPossible Issues\n\n\n\n\nNodes are node in Ready status\n\n\nkube-dns is crashing constantly\n\n\nSome of the systems services are not up\n\n\n\n\nMost of the times, kubernetes does self heal, unless its a issue with system resources not being adequate. Upgrading resources or launching it on bigger capacity VM/servers solves it. However, if the issues persist, you could try following techniques,\n\n\nTroubleshooting Tips\n\n\nCheck events\n\n\nkubectl get events\n\n\n\n\nCheck Logs\n\n\nkubectl get pods -n kube-system\n\n[get the name of the pod which has a problem]\n\nkubectl logs \npod\n -n kube-system\n\n\n\n\n\ne.g.\n\n\nroot@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994 -n kube-system\nError from server (BadRequest): a container name must be specified for pod kube-dns-545bc4bfd4-dh994, choose one of:\n[kubedns dnsmasq sidecar]\n\n\nroot@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994  kubedns  -n kube-system\nI1106 14:41:15.542409       1 dns.go:48] version: 1.14.4-2-g5584e04\nI1106 14:41:15.543487       1 server.go:70] Using\n\n....\n\n\n\n\n\nEnable Kubernetes Dashboard\n\n\nAfter the Pod networks is installled, We can install another add-on service which is Kubernetes Dashboard.\n\n\nInstalling Dashboard:\n\n\nkubectl apply -f https://gist.githubusercontent.com/initcron/32ff89394c881414ea7ef7f4d3a1d499/raw/baffda78ffdcaf8ece87a76fb2bb3fd767820a3f/kube-dashboard.yaml\n\n\n\n\n\nThis will create a pod for the Kubernetes Dashboard.\n\n\nTo access the Dashboard in th browser, run the below command\n\n\nkubectl describe svc kubernetes-dashboard -n kube-system\n\n\n\n\nSample output:\n\n\nkubectl describe svc kubernetes-dashboard -n kube-system\nName:                   kubernetes-dashboard\nNamespace:              kube-system\nLabels:                 app=kubernetes-dashboard\nSelector:               app=kubernetes-dashboard\nType:                   NodePort\nIP:                     10.98.148.82\nPort:                   \nunset\n 80/TCP\nNodePort:               \nunset\n 32756/TCP\nEndpoints:              10.40.0.1:9090\nSession Affinity:       None\n\n\n\n\nNow check for the node port, here it is 32756, and go to the browser,\n\n\nmasterip:32756\n\n\n\n\nThe Dashboard Looks like:", 
            "title": "Setup Kubernetes Cluster"
        }, 
        {
            "location": "/1. install_kubernetes/#compatibility", 
            "text": "Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.  The below steps are applicabe for the below mentioned OS     OS  Version  Codename      Ubuntu  16.04  Xenial", 
            "title": "Compatibility"
        }, 
        {
            "location": "/1. install_kubernetes/#base-setup", 
            "text": "Skip this step and scroll to Initializing Master if you have setup nodes with vagrant  On all nodes which would be part of this cluster, you need to do the base setup as described here,", 
            "title": "Base Setup"
        }, 
        {
            "location": "/1. install_kubernetes/#create-kubernetes-repository", 
            "text": "We need to create a repository to download Kubernetes.  curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -  cat  EOF   /etc/apt/sources.list.d/kubernetes.list\ndeb http://apt.kubernetes.io/ kubernetes-xenial main\nEOF", 
            "title": "Create Kubernetes Repository"
        }, 
        {
            "location": "/1. install_kubernetes/#installation-of-the-packages", 
            "text": "We should update the machines before installing so that we can update the repository.  apt-get update -y  Installing all the packages with dependencies:  apt-get install -y docker.io kubelet kubeadm kubectl kubernetes-cni  rm -rf /var/lib/kubelet/*", 
            "title": "Installation of the packages"
        }, 
        {
            "location": "/1. install_kubernetes/#setup-sysctl-configs", 
            "text": "In order for many container networks to work, the following needs to be enabled on each node.  sysctl net.bridge.bridge-nf-call-iptables=1  The above steps has to be followed in all the nodes.", 
            "title": "Setup sysctl configs"
        }, 
        {
            "location": "/1. install_kubernetes/#initializing-master", 
            "text": "This tutorial assumes  kube-01   as the master and used kubeadm as a tool to install and setup the cluster. This section also assumes that you are using vagrant based setup provided along with this tutorial. If not, please update the IP address of the master accordingly.  To initialize master, run this on kube-01  kubeadm init --apiserver-advertise-address 192.168.12.10 --pod-network-cidr=192.168.0.0/16", 
            "title": "Initializing Master"
        }, 
        {
            "location": "/1. install_kubernetes/#initialization-of-the-nodes-previously-minions", 
            "text": "After master being initialized, it should display the command which could be used on all worker/nodes to join the k8s cluster.  e.g.  kubeadm join --token c04797.8db60f6b2c0dd078 192.168.12.10:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0  Copy and paste it on all node.", 
            "title": "Initialization of the Nodes (Previously Minions)"
        }, 
        {
            "location": "/1. install_kubernetes/#troubleshooting-tip", 
            "text": "If you lose  the join token, you could retrieve it using  kubeadm token list  On successfully joining the master, you should see output similar to following,  root@kube-03:~# kubeadm join --token c04797.8db60f6b2c0dd078 159.203.170.84:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0\n[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.\n[preflight] Running pre-flight checks\n[discovery] Trying to connect to API Server  159.203.170.84:6443 \n[discovery] Created cluster-info discovery client, requesting info from  https://159.203.170.84:6443 \n[discovery] Requesting info from  https://159.203.170.84:6443  again to validate TLS against the pinned public key\n[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server  159.203.170.84:6443 \n[discovery] Successfully established connection with API Server  159.203.170.84:6443 \n[bootstrap] Detected server version: v1.8.2\n[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)\n\nNode join complete:\n* Certificate signing request sent to master and response\n  received.\n* Kubelet informed of new secure connection details.\n\nRun 'kubectl get nodes' on the master to see this machine join.", 
            "title": "Troubleshooting Tip"
        }, 
        {
            "location": "/1. install_kubernetes/#setup-the-admin-client-kubectl", 
            "text": "On Master Node  mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config", 
            "title": "Setup the admin client - Kubectl"
        }, 
        {
            "location": "/1. install_kubernetes/#installing-cni-with-weave", 
            "text": "Installing overlay network is necessary for the pods to communicate with each other across the hosts. It is necessary to do this before you try to deploy any applications to your cluster.  There are various overlay networking drivers available for kubernetes. We are going to use  Weave Net .  \nexport kubever=$(kubectl version | base64 | tr -d '\\n')\nkubectl apply -f  https://cloud.weave.works/k8s/net?k8s-version=$kubever", 
            "title": "Installing CNI with Weave"
        }, 
        {
            "location": "/1. install_kubernetes/#validating-the-setup", 
            "text": "You could validate the status of this cluster, health of pods and whether all the components are up or not by using a few or all of the following commands.  To check if nodes are ready  kubectl get nodes  [ Expected output ]  root@kube-01:~# kubectl get nodes\nNAME      STATUS    ROLES     AGE       VERSION\nkube-01   Ready     master    9m        v1.8.2\nkube-02   Ready      none     4m        v1.8.2\nkube-03   Ready      none     4m        v1.8.2  Additional Status Commands  kubectl cluster-info\n\nkubectl get pods -n kube-system\n\nkubectl get events  It will take a few minutes to have the cluster up and running with all the services.", 
            "title": "Validating the Setup"
        }, 
        {
            "location": "/1. install_kubernetes/#possible-issues", 
            "text": "Nodes are node in Ready status  kube-dns is crashing constantly  Some of the systems services are not up   Most of the times, kubernetes does self heal, unless its a issue with system resources not being adequate. Upgrading resources or launching it on bigger capacity VM/servers solves it. However, if the issues persist, you could try following techniques,  Troubleshooting Tips  Check events  kubectl get events  Check Logs  kubectl get pods -n kube-system\n\n[get the name of the pod which has a problem]\n\nkubectl logs  pod  -n kube-system  e.g.  root@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994 -n kube-system\nError from server (BadRequest): a container name must be specified for pod kube-dns-545bc4bfd4-dh994, choose one of:\n[kubedns dnsmasq sidecar]\n\n\nroot@kube-01:~# kubectl logs kube-dns-545bc4bfd4-dh994  kubedns  -n kube-system\nI1106 14:41:15.542409       1 dns.go:48] version: 1.14.4-2-g5584e04\nI1106 14:41:15.543487       1 server.go:70] Using\n\n....", 
            "title": "Possible Issues"
        }, 
        {
            "location": "/1. install_kubernetes/#enable-kubernetes-dashboard", 
            "text": "After the Pod networks is installled, We can install another add-on service which is Kubernetes Dashboard.  Installing Dashboard:  kubectl apply -f https://gist.githubusercontent.com/initcron/32ff89394c881414ea7ef7f4d3a1d499/raw/baffda78ffdcaf8ece87a76fb2bb3fd767820a3f/kube-dashboard.yaml  This will create a pod for the Kubernetes Dashboard.  To access the Dashboard in th browser, run the below command  kubectl describe svc kubernetes-dashboard -n kube-system  Sample output:  kubectl describe svc kubernetes-dashboard -n kube-system\nName:                   kubernetes-dashboard\nNamespace:              kube-system\nLabels:                 app=kubernetes-dashboard\nSelector:               app=kubernetes-dashboard\nType:                   NodePort\nIP:                     10.98.148.82\nPort:                    unset  80/TCP\nNodePort:                unset  32756/TCP\nEndpoints:              10.40.0.1:9090\nSession Affinity:       None  Now check for the node port, here it is 32756, and go to the browser,  masterip:32756  The Dashboard Looks like:", 
            "title": "Enable Kubernetes Dashboard"
        }, 
        {
            "location": "/configs/", 
            "text": "In this lesson we are going to cover the following topics\n\n\n\n\nSetting up monitors\n\n\nConfigs\n\n\nContext\n\n\nNamespaces\n\n\n\n\nSetup monitoring console for Kubernetes\n\n\nUnix \nscreen\n is a great utility for a devops professional. You could setup a simple monitoring for kubernetes cluster using a \nscreenrc\n script as follows on kube-01 node (node where \nkubectl\n is configured)\n\n\nfile: k8s-code/monitoring/rc.screenrc\n\n\nscreen watch -n 1 kubectl get pods\nsplit\nfocus down\nscreen watch -n 1 kubectl get rc\nfocus bottom\n\n\n\n\nOpen a dedicated terminal to run this utility.  Launch it using\n\n\nscreen -c monitoring/rc.screenrc\n\n\n\n\n\nListing Configurations\n\n\nCheck current config\n\n\nkubectl config view\n\n\n\n\nYou could also examine the current configs in file \ncat ~/.kube/config\n\n\nCreating a dev namespace\n\n\nNamespaces offers separation of resources running on the same physical infrastructure into virtual clusters. It is typically useful in mid to large scale environments with multiple projects, teams and need separate scopes. It could also be useful to map to your workflow stages e.g. dev, stage, prod.   \n\n\nLets create a namespace called \ndev\n  \n\n\nfile: dev_ns.yaml\n\n\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: dev\n  labels:\n    name: dev\n\n\n\n\nTo create namespace\n\n\nkubectl apply -f dev_ns.yaml\n\n\n\n\nAnd switch to it\n\n\nkubectl config set-context $(kubectl config current-context) --namespace=dev", 
            "title": "Configuring Cluster"
        }, 
        {
            "location": "/configs/#setup-monitoring-console-for-kubernetes", 
            "text": "Unix  screen  is a great utility for a devops professional. You could setup a simple monitoring for kubernetes cluster using a  screenrc  script as follows on kube-01 node (node where  kubectl  is configured)  file: k8s-code/monitoring/rc.screenrc  screen watch -n 1 kubectl get pods\nsplit\nfocus down\nscreen watch -n 1 kubectl get rc\nfocus bottom  Open a dedicated terminal to run this utility.  Launch it using  screen -c monitoring/rc.screenrc", 
            "title": "Setup monitoring console for Kubernetes"
        }, 
        {
            "location": "/configs/#listing-configurations", 
            "text": "Check current config  kubectl config view  You could also examine the current configs in file  cat ~/.kube/config", 
            "title": "Listing Configurations"
        }, 
        {
            "location": "/configs/#creating-a-dev-namespace", 
            "text": "Namespaces offers separation of resources running on the same physical infrastructure into virtual clusters. It is typically useful in mid to large scale environments with multiple projects, teams and need separate scopes. It could also be useful to map to your workflow stages e.g. dev, stage, prod.     Lets create a namespace called  dev     file: dev_ns.yaml  kind: Namespace\napiVersion: v1\nmetadata:\n  name: dev\n  labels:\n    name: dev  To create namespace  kubectl apply -f dev_ns.yaml  And switch to it  kubectl config set-context $(kubectl config current-context) --namespace=dev", 
            "title": "Creating a dev namespace"
        }, 
        {
            "location": "/deploying_pods/", 
            "text": "Deploying Pods\n\n\nLife of a pod\n\n\n\n\nPending : in progress\n\n\nRunning\n\n\nSucceeded : successfully exited\n\n\nFailed\n\n\nUnknown\n\n\n\n\nProbes\n\n\n\n\nlivenessProbe : Containers are Alive\n\n\nreadinessProbe : Ready to Serve Traffic\n\n\n\n\nResource Configs\n\n\nEach entity created with kubernetes is a resource including pod, service, deployments, replication controller etc. Resources can be defined as YAML or JSON.  Here is the syntax to create a YAML specification.\n\n\nAKMS\n =\n Resource Configs Specs\n\n\napiVersion: v1\nkind:\nmetadata:\nspec:\n\n\n\n\nSpec Schema: https://kubernetes.io/docs/user-guide/pods/multi-container/\n\n\nCommon Configurations\n\n\nThrougout this tutorial, we would be deploying differnt components of  example voting application. Lets assume we are deploying it in a \ndev\n environment.  Lets create the common specs for this app with the AKMS schema discussed above.\n\n\nfile: common.yml\n\n\napiVersion: v1\nkind:\nmetadata:\n  name: vote\n  labels:\n    app: vote\n    role: ui\n    tier: front\nspec:\n\n\n\n\nLets now create the  Pod config by adding the kind and specs to above schema.\n\n\nFilename: vote_pod.yaml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: vote\n  labels:\n    app: vote\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: vote\n      image: schoolofdevops/vote:latest\n      ports:\n        - containerPort: 80\n\n\n\n\nLaunching and operating a Pod\n\n\nSyntax:\n\n\n kubectl apply -f FILE\n\n\n\n\nTo Launch pod using configs above,\n\n\nkubectl apply -f vote_pod.yaml\n\n\n\n\n\nTo view pods\n\n\nkubectl get pods\n\nkubectl get pods vote\n\n\n\n\nTo get detailed info\n\n\nkubectl describe pods vote\n\n\n\n\n[Output:]\n\n\nName:           vote\nNamespace:      default\nNode:           kube-3/192.168.0.80\nStart Time:     Tue, 07 Feb 2017 16:16:40 +0000\nLabels:         app=voting\nStatus:         Running\nIP:             10.40.0.2\nControllers:    \nnone\n\nContainers:\n  vote:\n    Container ID:       docker://48304b35b9457d627b341e424228a725d05c2ed97cc9970bbff32a1b365d9a5d\n    Image:              schoolofdevops/vote:latest\n    Image ID:           docker-pullable://schoolofdevops/vote@sha256:3d89bfc1993d4630a58b831a6d44ef73d2be76a7862153e02e7a7c0cf2936731\n    Port:               80/TCP\n    State:              Running\n      Started:          Tue, 07 Feb 2017 16:16:52 +0000\n    Ready:              True\n    Restart Count:      0\n    Volume Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2n6j1 (ro)\n    Environment Variables:      \nnone\n\nConditions:\n  Type          Status\n  Initialized   True\n  Ready         True\n  PodScheduled  True\nVolumes:\n  default-token-2n6j1:\n    Type:       Secret (a volume populated by a Secret)\n    SecretName: default-token-2n6j1\nQoS Class:      BestEffort\nTolerations:    \nnone\n\nEvents:\n  FirstSeen     LastSeen        Count   From                    SubObjectPath           Type            Reason          Message\n  ---------     --------        -----   ----                    -------------           --------        ------          -------\n  21s           21s             1       {default-scheduler }                            Normal          Scheduled       Successfully assigned vote to kube-3\n  20s           20s             1       {kubelet kube-3}        spec.containers{vote}   Normal          Pulling         pulling image \nschoolofdevops/vote:latest\n\n  10s           10s             1       {kubelet kube-3}        spec.containers{vote}   Normal          Pulled          Successfully pulled image \nschoolofdevops/vote:latest\n\n  9s            9s              1       {kubelet kube-3}        spec.containers{vote}   Normal          Created         Created container with docker id 48304b35b945; Security:[seccomp=unconfined]\n  9s            9s              1       {kubelet kube-3}        spec.containers{vote}   Normal          Started         Started container with docker id 48304b35b945\n\n\n\n\nCommands to operate the pod\n\n\nkubectl exec -it vote ps sh\n\nkubectl exec -it vote  sh\n\nkubectl logs vote\n\n\n\n\n\ndelete\n\n\nkubectl delete pod vote\n\nkubectl get pods\n\n\n\n\nAttach a Volume to the Pod\n\n\nLets create a pod for database and attach a volume to it. To achieve this we will need to\n\n\n\n\ncreate a \nvolumes\n definition\n\n\nattach volume to container using \nVolumeMounts\n property\n\n\n\n\nVolumes are of two types:\n  * emptyDir\n  * hostPath\n\n\nFile: db_pod.yaml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: db\n  labels:\n    app: postgres\n    role: database\n    tier: back\nspec:\n  containers:\n    - name: db\n      image: postgres:9.4\n      ports:\n        - containerPort: 5432\n      volumeMounts:\n      - name: db-data\n        mountPath: /var/lib/postgresql/data\n  volumes:\n  - name: db-data\n    emptyDir: {}\n\n\n\n\n\nTo create this pod,\n\n\nkubectl apply -f db_pod.yaml\n\nkubectl describe pod db\n\nkubectl get events\n\n\n\n\nSelecting Node to run on\n\n\nkubectl get nodes --show-labels\n\nkubectl label nodes \nnode-name\n rack=1\n\nkubectl get nodes --show-labels\n\n\n\n\n\nUpdate pod definition with nodeSelector\n\n\nfile: vote_pod.yml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: vote\n  labels:\n    app: vote\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: vote\n      image: schoolofdevops/vote:latest\n      ports:\n        - containerPort: 80\n  nodeSelector:\n    rack: '1'\n\n\n\n\nFor this change, pod needs to be re created.\n\n\nkubectl apply -f vote_pod.yaml\n\n\n\n\nCreating Multi Container Pods\n\n\nfile: multi_container_pod.yml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: nginx\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n      ports:\n        - containerPort: 80\n      volumeMounts:\n      - name: data\n        mountPath: /opt/d1\n    - name: loop\n      image: schoolofdevops/loop\n      volumeMounts:\n      - name: data\n        mountPath: /opt/d1\n  volumes:\n  - name: data\n    emptyDir: {}\n\n\n\n\nTo create this pod\n\n\nkubectl apply -f multi_container_pod.yml\n\n\n\n\nCheck Status\n\n\nroot@kube-01:~# kubectl get pods\nNAME      READY     STATUS              RESTARTS   AGE\nnginx     0/2       ContainerCreating   0          7s\nvote      1/1       Running             0          3m\n\n\n\n\nChecking logs, logging in\n\n\nkubectl logs  web  -c loop\nkubectl logs  web  -c nginx\n\nkubectl exec -it web  sh  -c nginx\nkubectl exec -it web  sh  -c loop\n\n\n\n\n\nExercise\n\n\nCreate a pod definition for redis and deploy.\n\n\nReading List :\n\n\nNode Selectors, Affinity\nhttps://kubernetes.io/docs/concepts/configuration/assign-pod-node/", 
            "title": "Launching Pods"
        }, 
        {
            "location": "/deploying_pods/#deploying-pods", 
            "text": "Life of a pod   Pending : in progress  Running  Succeeded : successfully exited  Failed  Unknown", 
            "title": "Deploying Pods"
        }, 
        {
            "location": "/deploying_pods/#probes", 
            "text": "livenessProbe : Containers are Alive  readinessProbe : Ready to Serve Traffic", 
            "title": "Probes"
        }, 
        {
            "location": "/deploying_pods/#resource-configs", 
            "text": "Each entity created with kubernetes is a resource including pod, service, deployments, replication controller etc. Resources can be defined as YAML or JSON.  Here is the syntax to create a YAML specification.  AKMS  =  Resource Configs Specs  apiVersion: v1\nkind:\nmetadata:\nspec:  Spec Schema: https://kubernetes.io/docs/user-guide/pods/multi-container/", 
            "title": "Resource Configs"
        }, 
        {
            "location": "/deploying_pods/#common-configurations", 
            "text": "Througout this tutorial, we would be deploying differnt components of  example voting application. Lets assume we are deploying it in a  dev  environment.  Lets create the common specs for this app with the AKMS schema discussed above.  file: common.yml  apiVersion: v1\nkind:\nmetadata:\n  name: vote\n  labels:\n    app: vote\n    role: ui\n    tier: front\nspec:  Lets now create the  Pod config by adding the kind and specs to above schema.  Filename: vote_pod.yaml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: vote\n  labels:\n    app: vote\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: vote\n      image: schoolofdevops/vote:latest\n      ports:\n        - containerPort: 80", 
            "title": "Common Configurations"
        }, 
        {
            "location": "/deploying_pods/#launching-and-operating-a-pod", 
            "text": "Syntax:   kubectl apply -f FILE  To Launch pod using configs above,  kubectl apply -f vote_pod.yaml  To view pods  kubectl get pods\n\nkubectl get pods vote  To get detailed info  kubectl describe pods vote  [Output:]  Name:           vote\nNamespace:      default\nNode:           kube-3/192.168.0.80\nStart Time:     Tue, 07 Feb 2017 16:16:40 +0000\nLabels:         app=voting\nStatus:         Running\nIP:             10.40.0.2\nControllers:     none \nContainers:\n  vote:\n    Container ID:       docker://48304b35b9457d627b341e424228a725d05c2ed97cc9970bbff32a1b365d9a5d\n    Image:              schoolofdevops/vote:latest\n    Image ID:           docker-pullable://schoolofdevops/vote@sha256:3d89bfc1993d4630a58b831a6d44ef73d2be76a7862153e02e7a7c0cf2936731\n    Port:               80/TCP\n    State:              Running\n      Started:          Tue, 07 Feb 2017 16:16:52 +0000\n    Ready:              True\n    Restart Count:      0\n    Volume Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2n6j1 (ro)\n    Environment Variables:       none \nConditions:\n  Type          Status\n  Initialized   True\n  Ready         True\n  PodScheduled  True\nVolumes:\n  default-token-2n6j1:\n    Type:       Secret (a volume populated by a Secret)\n    SecretName: default-token-2n6j1\nQoS Class:      BestEffort\nTolerations:     none \nEvents:\n  FirstSeen     LastSeen        Count   From                    SubObjectPath           Type            Reason          Message\n  ---------     --------        -----   ----                    -------------           --------        ------          -------\n  21s           21s             1       {default-scheduler }                            Normal          Scheduled       Successfully assigned vote to kube-3\n  20s           20s             1       {kubelet kube-3}        spec.containers{vote}   Normal          Pulling         pulling image  schoolofdevops/vote:latest \n  10s           10s             1       {kubelet kube-3}        spec.containers{vote}   Normal          Pulled          Successfully pulled image  schoolofdevops/vote:latest \n  9s            9s              1       {kubelet kube-3}        spec.containers{vote}   Normal          Created         Created container with docker id 48304b35b945; Security:[seccomp=unconfined]\n  9s            9s              1       {kubelet kube-3}        spec.containers{vote}   Normal          Started         Started container with docker id 48304b35b945  Commands to operate the pod  kubectl exec -it vote ps sh\n\nkubectl exec -it vote  sh\n\nkubectl logs vote  delete  kubectl delete pod vote\n\nkubectl get pods", 
            "title": "Launching and operating a Pod"
        }, 
        {
            "location": "/deploying_pods/#attach-a-volume-to-the-pod", 
            "text": "Lets create a pod for database and attach a volume to it. To achieve this we will need to   create a  volumes  definition  attach volume to container using  VolumeMounts  property   Volumes are of two types:\n  * emptyDir\n  * hostPath  File: db_pod.yaml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: db\n  labels:\n    app: postgres\n    role: database\n    tier: back\nspec:\n  containers:\n    - name: db\n      image: postgres:9.4\n      ports:\n        - containerPort: 5432\n      volumeMounts:\n      - name: db-data\n        mountPath: /var/lib/postgresql/data\n  volumes:\n  - name: db-data\n    emptyDir: {}  To create this pod,  kubectl apply -f db_pod.yaml\n\nkubectl describe pod db\n\nkubectl get events", 
            "title": "Attach a Volume to the Pod"
        }, 
        {
            "location": "/deploying_pods/#selecting-node-to-run-on", 
            "text": "kubectl get nodes --show-labels\n\nkubectl label nodes  node-name  rack=1\n\nkubectl get nodes --show-labels  Update pod definition with nodeSelector  file: vote_pod.yml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: vote\n  labels:\n    app: vote\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: vote\n      image: schoolofdevops/vote:latest\n      ports:\n        - containerPort: 80\n  nodeSelector:\n    rack: '1'  For this change, pod needs to be re created.  kubectl apply -f vote_pod.yaml", 
            "title": "Selecting Node to run on"
        }, 
        {
            "location": "/deploying_pods/#creating-multi-container-pods", 
            "text": "file: multi_container_pod.yml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    app: nginx\n    role: ui\n    tier: front\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n      ports:\n        - containerPort: 80\n      volumeMounts:\n      - name: data\n        mountPath: /opt/d1\n    - name: loop\n      image: schoolofdevops/loop\n      volumeMounts:\n      - name: data\n        mountPath: /opt/d1\n  volumes:\n  - name: data\n    emptyDir: {}  To create this pod  kubectl apply -f multi_container_pod.yml  Check Status  root@kube-01:~# kubectl get pods\nNAME      READY     STATUS              RESTARTS   AGE\nnginx     0/2       ContainerCreating   0          7s\nvote      1/1       Running             0          3m  Checking logs, logging in  kubectl logs  web  -c loop\nkubectl logs  web  -c nginx\n\nkubectl exec -it web  sh  -c nginx\nkubectl exec -it web  sh  -c loop", 
            "title": "Creating Multi Container Pods"
        }, 
        {
            "location": "/deploying_pods/#exercise", 
            "text": "Create a pod definition for redis and deploy.", 
            "title": "Exercise"
        }, 
        {
            "location": "/deploying_pods/#reading-list", 
            "text": "Node Selectors, Affinity\nhttps://kubernetes.io/docs/concepts/configuration/assign-pod-node/", 
            "title": "Reading List :"
        }, 
        {
            "location": "/2. kubernetes_deployment/", 
            "text": "Creating a Deployment\n\n\nA Deployment is a higher level abstraction which sits on top of replica sets and allows you to manage the way applications are deployed, rolled back at a controlled rate.\n\n\nFeatures\n  * Rollout a Replicaset\n  * Deploy a new version : Creates a new replica set every time, moves pods from RS(n) to RS(n+1)\n  * Rollback to previous RS\n\n  * Auto Scaling\n  * Pause Deployments\n\n\nFile: vote_dep.yaml\n\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: vote\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: front\n      app: vote\n    matchExpressions:\n      - {key: tier, operator: In, values: [front]}\n  minReadySeconds: 20\n  template:\n    metadata:\n      labels:\n        app: vote\n        role: ui\n        tier: front\n    spec:\n      containers:\n      - image: schoolofdevops/vote\n        imagePullPolicy: Always\n        name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP\n      restartPolicy: Always\n\n\n\n\nAnd save the file.\n\n\nNow create the Deployment\n\n\nkubectl apply -f vote_dep.yaml --record\n\n\n\n\nNow the deployment is created. To check it,\n\n\nkubectl rollout status deployment/vote\nkubectl get deployment\nkubectl get rs\nkubectl get pods --show-labels\n\n\n\n\nSample Output\n\n\nkubectl get deployments\nNAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nvote   3         3         3            1           3m\n\n\n\n\nScaling a deployment\n\n\nTo scale a deployment in Kubernetes:\n\n\nkubectl scale deployment/vote --replicas=5\n\n\n\n\nSample output:\n\n\nkubectl scale deployment/vote --replicas=5\ndeployment \nvote\n scaled\n\n\n\n\nRolling updates with deployments\n\n\nUpdate the version of the image in voteloyment.yaml\n\n\nFile: vote_dep.yaml\n\n\n...\n        app: vote\n    spec:\n      containers:\n      - image: schoolofdevops/vote:movies\n\n\n\n\n\nApply Changes and monitor the rollout\n\n\nkubectl apply -f voteloyment.yaml\nkubectl rollout status deployment/vote\n\n\n\n\nRolling Back a Failed Update\n\n\nLets update the image to a tag which us non existant. We introduce this intentional error to fail fail the deployment.\n\n\nFile: voteloyment.yaml\n\n\n...\n    app: vote\n    spec:\n      containers:\n      - image: schoolofdevops/vote:movi\n\n\n\n\n\nDo a new rollout and monitor\n\n\nkubectl apply -f voteloyment.yaml\nkubectl rollout status deployment/vote\n\n\n\n\nAlso watch the pod status which might look like\n\n\nvote-3040199436-sdq17   1/1       Running            0          9m\nvote-4086029260-0vjjb   0/1       ErrImagePull       0          16s\nvote-4086029260-zvgmd   0/1       ImagePullBackOff   0          15s\nvote-rc-fsdsd               1/1       Running            0          27m\nvote-rc-mcxs5               1/1       Running            0\n\n\n\n\nTo get the revision history and details  \n\n\nkubectl rollout history deployment/vote\nkubectl rollout history deployment/vote --revision=x\n[replace x with the latest revision]\n\n\n\n\n[Sample Output]\n\n\nroot@kube-01:~# kubectl rollout history deployment/vote\ndeployments \nvote\n\nREVISION    CHANGE-CAUSE\n1       kubectl scale deployment/vote --replicas=5\n3       \nnone\n\n6       \nnone\n\n7       \nnone\n\n\nroot@kube-01:~# kubectl rollout history deployment/vote --revision=7\ndeployments \nvote\n with revision #7\nPod Template:\n  Labels:   app=vote\n    env=dev\n    pod-template-hash=4086029260\n    role=ui\n    stack=voting\n    tier=front\n  Containers:\n   vote:\n    Image:  schoolofdevops/vote:movi\n    Port:   80/TCP\n    Environment:    \nnone\n\n    Mounts: \nnone\n\n  Volumes:  \nnone\n\n\n\n\n\nTo undo rollout,\n\n\nkubectl rollout undo deployment/vote\n\n\n\n\nor\n\n\nkubectl rollout undo deployment/vote --to-revision=1\nkubectl get rs\nkubectl describe deployment vote", 
            "title": "Creating Deployments"
        }, 
        {
            "location": "/2. kubernetes_deployment/#creating-a-deployment", 
            "text": "A Deployment is a higher level abstraction which sits on top of replica sets and allows you to manage the way applications are deployed, rolled back at a controlled rate.  Features\n  * Rollout a Replicaset\n  * Deploy a new version : Creates a new replica set every time, moves pods from RS(n) to RS(n+1)\n  * Rollback to previous RS \n  * Auto Scaling\n  * Pause Deployments  File: vote_dep.yaml  apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: vote\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: front\n      app: vote\n    matchExpressions:\n      - {key: tier, operator: In, values: [front]}\n  minReadySeconds: 20\n  template:\n    metadata:\n      labels:\n        app: vote\n        role: ui\n        tier: front\n    spec:\n      containers:\n      - image: schoolofdevops/vote\n        imagePullPolicy: Always\n        name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP\n      restartPolicy: Always  And save the file.  Now create the Deployment  kubectl apply -f vote_dep.yaml --record  Now the deployment is created. To check it,  kubectl rollout status deployment/vote\nkubectl get deployment\nkubectl get rs\nkubectl get pods --show-labels  Sample Output  kubectl get deployments\nNAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nvote   3         3         3            1           3m", 
            "title": "Creating a Deployment"
        }, 
        {
            "location": "/2. kubernetes_deployment/#scaling-a-deployment", 
            "text": "To scale a deployment in Kubernetes:  kubectl scale deployment/vote --replicas=5  Sample output:  kubectl scale deployment/vote --replicas=5\ndeployment  vote  scaled", 
            "title": "Scaling a deployment"
        }, 
        {
            "location": "/2. kubernetes_deployment/#rolling-updates-with-deployments", 
            "text": "Update the version of the image in voteloyment.yaml  File: vote_dep.yaml  ...\n        app: vote\n    spec:\n      containers:\n      - image: schoolofdevops/vote:movies  Apply Changes and monitor the rollout  kubectl apply -f voteloyment.yaml\nkubectl rollout status deployment/vote", 
            "title": "Rolling updates with deployments"
        }, 
        {
            "location": "/2. kubernetes_deployment/#rolling-back-a-failed-update", 
            "text": "Lets update the image to a tag which us non existant. We introduce this intentional error to fail fail the deployment.  File: voteloyment.yaml  ...\n    app: vote\n    spec:\n      containers:\n      - image: schoolofdevops/vote:movi  Do a new rollout and monitor  kubectl apply -f voteloyment.yaml\nkubectl rollout status deployment/vote  Also watch the pod status which might look like  vote-3040199436-sdq17   1/1       Running            0          9m\nvote-4086029260-0vjjb   0/1       ErrImagePull       0          16s\nvote-4086029260-zvgmd   0/1       ImagePullBackOff   0          15s\nvote-rc-fsdsd               1/1       Running            0          27m\nvote-rc-mcxs5               1/1       Running            0  To get the revision history and details    kubectl rollout history deployment/vote\nkubectl rollout history deployment/vote --revision=x\n[replace x with the latest revision]  [Sample Output]  root@kube-01:~# kubectl rollout history deployment/vote\ndeployments  vote \nREVISION    CHANGE-CAUSE\n1       kubectl scale deployment/vote --replicas=5\n3        none \n6        none \n7        none \n\nroot@kube-01:~# kubectl rollout history deployment/vote --revision=7\ndeployments  vote  with revision #7\nPod Template:\n  Labels:   app=vote\n    env=dev\n    pod-template-hash=4086029260\n    role=ui\n    stack=voting\n    tier=front\n  Containers:\n   vote:\n    Image:  schoolofdevops/vote:movi\n    Port:   80/TCP\n    Environment:     none \n    Mounts:  none \n  Volumes:   none   To undo rollout,  kubectl rollout undo deployment/vote  or  kubectl rollout undo deployment/vote --to-revision=1\nkubectl get rs\nkubectl describe deployment vote", 
            "title": "Rolling Back a Failed Update"
        }, 
        {
            "location": "/exposing_app_with_service/", 
            "text": "Exposing Application with  a Service\n\n\nTypes of Services:\n  * ClusterIP\n  * NodePort\n  * LoadBalancer\n  * ExternalName\n\n\nkubectl get pods\nkubectl get svc\n\n\n\n\nSample Output:\n\n\nNAME                READY     STATUS    RESTARTS   AGE\nvoting-appp-1j52x   1/1       Running   0          12m\nvoting-appp-pr2xz   1/1       Running   0          9m\nvoting-appp-qpxbm   1/1       Running   0          15m\n\n\n\n\nFilename: vote_svc.yaml\n\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    role: svc\n    tier: front\n  name: vote-svc\n  namespace: dev\nspec:\n  selector:\n    app: vote\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  type: NodePort\n\n\n\n\nSave the file.\n\n\nNow to create a service:\n\n\nkubectl create -f vote_svc.yaml\nkubectl get svc\n\n\n\n\nNow to check which port the pod is connected\n\n\nkubectl describe service vote-svc\n\n\n\n\nCheck for the Nodeport here\n\n\nSample Output\n\n\nName:                   vote-svc\nNamespace:              dev\nLabels:                 app=vote\nSelector:               app=vote\nType:                   NodePort\nIP:                     10.99.147.158\nPort:                   \nunset\n 80/TCP\nNodePort:               \nunset\n 30308/TCP\nEndpoints:              10.40.0.2:80,10.40.0.3:80,10.40.0.4:80 + 1 more...\nSession Affinity:       None\nNo events.\n\n\n\n\nGo to browser and check hostip:NodePort\n\n\nHere the node port is 30308.\n\n\nSample output will be:", 
            "title": "Service Endpoints"
        }, 
        {
            "location": "/exposing_app_with_service/#exposing-application-with-a-service", 
            "text": "Types of Services:\n  * ClusterIP\n  * NodePort\n  * LoadBalancer\n  * ExternalName  kubectl get pods\nkubectl get svc  Sample Output:  NAME                READY     STATUS    RESTARTS   AGE\nvoting-appp-1j52x   1/1       Running   0          12m\nvoting-appp-pr2xz   1/1       Running   0          9m\nvoting-appp-qpxbm   1/1       Running   0          15m  Filename: vote_svc.yaml  ---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    role: svc\n    tier: front\n  name: vote-svc\n  namespace: dev\nspec:\n  selector:\n    app: vote\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  type: NodePort  Save the file.  Now to create a service:  kubectl create -f vote_svc.yaml\nkubectl get svc  Now to check which port the pod is connected  kubectl describe service vote-svc  Check for the Nodeport here  Sample Output  Name:                   vote-svc\nNamespace:              dev\nLabels:                 app=vote\nSelector:               app=vote\nType:                   NodePort\nIP:                     10.99.147.158\nPort:                    unset  80/TCP\nNodePort:                unset  30308/TCP\nEndpoints:              10.40.0.2:80,10.40.0.3:80,10.40.0.4:80 + 1 more...\nSession Affinity:       None\nNo events.  Go to browser and check hostip:NodePort  Here the node port is 30308.  Sample output will be:", 
            "title": "Exposing Application with  a Service"
        }, 
        {
            "location": "/5. Rolling Update with Deployment/", 
            "text": "Rolling Updates with Deployments\n\n\nA rolling update works by:\n1. Creating a new replication controller with the updated configuration.\n2. Increasing/decreasing the replica count on the new and old controllers until the correct number of replicas is reachedor by updating the version of the pod which is running.\n\n\nCreating a Deployment\n\n\nCreate a yaml file for the deployment.\n\n\nWe already have a yaml file to create a deployment \"vote_dep.yaml\"\nNow create the Deployment\n\n\nkubectl create -f vote_dep.yaml --record\n\n\n\n\nNow the deployment is created. To check it,\n\n\nkubectl get deployment\n\n\n\n\nSample Output\n\n\nkubectl get deployment\nNAME          DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nvoting-appp   1         1         1            1           19s\n\n\n\n\nCreating a Service for it\n\n\nNow to create a service:\n\n\nkubectl create -f voting-app-service.yaml --record\n\n\n\n\nSample Output:\n\n\nkubectl create -f voting-app-service.yaml --record\nservice \nvoting-appp\n created\n\n\n\n\nNow to check which port the pod is connected\n\n\nkubectl describe service voting-appp\n\n\n\n\nCheck for the Nodeport here\n\n\nSample Output\n\n\nkubectl describe service voting-appp\nName:                   voting-appp\nNamespace:              default\nLabels:                 run=voting-appp\nSelector:               run=voting-appp\nType:                   LoadBalancer\nIP:                     10.107.126.18\nPort:                   \nunset\n 80/TCP\nNodePort:               \nunset\n 30410/TCP\nEndpoints:              10.40.0.23:80\nSession Affinity:       None\nNo events.\n\n\n\n\nGo to browser and check hostip:NodePort\n\n\nHere the node port is 30410.\n\n\nSample output will be:\n\n\n\n\nRolling update\n\n\nNow we can see in the RC yaml file that the pods are running with the version of image : venkatsudharsanam/votingapp-python:8.0.0.\n\n\nLets try upgrading our pods to the version venkatsudharsanam/votingapp-python:10.0.0 without deleting the RC or the pods.\nThat's how the rolling updates helps in Kubernetes.\n\n\nCreate a new file\n\n\nvi voting-app-deployment1.yaml\n\n\n\n\nNow paste the below line in the yaml file\n\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: vote\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: front\n      app: vote\n    matchExpressions:\n      - {key: tier, operator: In, values: [front]}\n  minReadySeconds: 20\n  template:\n    metadata:\n      labels:\n        app: vote\n        role: ui\n        tier: front\n    spec:\n      containers:\n      - image: schoolofdevops/vote\n        imagePullPolicy: Always\n        name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP\n\n\n\n\nAnd save the file.\n\n\nNow to Roll out our update:\n\n\nkubectl apply -f voting-app-deployment1.yaml --record\n\n\n\n\nNow go to the browser and reload the page. You will see the below output:\n\n\n\n\nNow delete all the deployment and services", 
            "title": "Rollouts, Rollbacks and Rolling Updates"
        }, 
        {
            "location": "/5. Rolling Update with Deployment/#rolling-updates-with-deployments", 
            "text": "A rolling update works by:\n1. Creating a new replication controller with the updated configuration.\n2. Increasing/decreasing the replica count on the new and old controllers until the correct number of replicas is reachedor by updating the version of the pod which is running.", 
            "title": "Rolling Updates with Deployments"
        }, 
        {
            "location": "/5. Rolling Update with Deployment/#creating-a-deployment", 
            "text": "Create a yaml file for the deployment.  We already have a yaml file to create a deployment \"vote_dep.yaml\"\nNow create the Deployment  kubectl create -f vote_dep.yaml --record  Now the deployment is created. To check it,  kubectl get deployment  Sample Output  kubectl get deployment\nNAME          DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nvoting-appp   1         1         1            1           19s", 
            "title": "Creating a Deployment"
        }, 
        {
            "location": "/5. Rolling Update with Deployment/#creating-a-service-for-it", 
            "text": "Now to create a service:  kubectl create -f voting-app-service.yaml --record  Sample Output:  kubectl create -f voting-app-service.yaml --record\nservice  voting-appp  created  Now to check which port the pod is connected  kubectl describe service voting-appp  Check for the Nodeport here  Sample Output  kubectl describe service voting-appp\nName:                   voting-appp\nNamespace:              default\nLabels:                 run=voting-appp\nSelector:               run=voting-appp\nType:                   LoadBalancer\nIP:                     10.107.126.18\nPort:                    unset  80/TCP\nNodePort:                unset  30410/TCP\nEndpoints:              10.40.0.23:80\nSession Affinity:       None\nNo events.  Go to browser and check hostip:NodePort  Here the node port is 30410.  Sample output will be:", 
            "title": "Creating a Service for it"
        }, 
        {
            "location": "/5. Rolling Update with Deployment/#rolling-update", 
            "text": "Now we can see in the RC yaml file that the pods are running with the version of image : venkatsudharsanam/votingapp-python:8.0.0.  Lets try upgrading our pods to the version venkatsudharsanam/votingapp-python:10.0.0 without deleting the RC or the pods.\nThat's how the rolling updates helps in Kubernetes.  Create a new file  vi voting-app-deployment1.yaml  Now paste the below line in the yaml file  apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: vote\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: front\n      app: vote\n    matchExpressions:\n      - {key: tier, operator: In, values: [front]}\n  minReadySeconds: 20\n  template:\n    metadata:\n      labels:\n        app: vote\n        role: ui\n        tier: front\n    spec:\n      containers:\n      - image: schoolofdevops/vote\n        imagePullPolicy: Always\n        name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP  And save the file.  Now to Roll out our update:  kubectl apply -f voting-app-deployment1.yaml --record  Now go to the browser and reload the page. You will see the below output:   Now delete all the deployment and services", 
            "title": "Rolling update"
        }, 
        {
            "location": "/6. Kubernetes Autoscaling/", 
            "text": "Kubernetes Horizonntal Pod Autoscaling\n\n\nWith Horizontal Pod Autoscaling, Kubernetes automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with alpha support, on some other, application-provided metrics).\n\n\nThe Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user\n\n\nPrerequisites\n\n\nHeapster monitoring needs to be deployed in the cluster as Horizontal Pod Autoscaler uses it to collect metrics.\n\n\nDeploying Heapster\n\n\nGo to the below directory and create the deployment and services.\n\n\ngit clone https://github.com/kubernetes/heapster.git\ncd heapster\nkubectl apply -f deploy/kube-config/influxdb/\nkubectl apply -f deploy/kube-config/rbac/heapster-rbac.yaml\n\n\n\n\nValidate that heapster, influxdb and grafana are started\n\n\nkubectl get pods -n kube-system\nkubectl get svc -n kube-system\n\n\n\n\n\nNow this will deploy the heapster monitoring.\n\n\nRun \n expose php-apache server\n\n\nTo demonstrate Horizontal Pod Autoscaler we will use a custom docker image based on the php-apache image\n\n\nkubectl run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80  \n\n\n\n\nSample Output\n\n\nkubectl run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80\nservice \nphp-apache\n created\ndeployment \nphp-apache\n created\n\n\n\n\nTo verify the created pod:\n\n\nkubectl get pods\n\n\n\n\nWait untill the pod changes to running state.\n\n\nCreate Horizontal Pod Autoscaler\n\n\nNow that the server is running, we will create the autoscaler using kubectl autoscale. The following command will create a Horizontal Pod Autoscaler that maintains between 1 and 10 replicas of the Pods controlled by the php-apache deployment we created in the first step of these instructions.\n\n\nkubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10\n\n\n\n\nSample Output\n\n\nkubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10\ndeployment \nphp-apache\n autoscaled\n\n\n\n\nWe may check the current status of autoscaler by running:\n\n\nkubectl get hpa\n\n\n\n\nSample Output:\n\n\nkubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache   50%       0%        1         10        18s\n\n\n\n\nIncrease load\n\n\nNow we can increase the load and trying testing what will happen.\nWe will start a container, and send an infinite loop of queries to the php-apache service\n\n\nkubectl run -i --tty -n dev load-generator --image=busybox /bin/sh\n\nHit enter for command prompt\n\nwhile true; do wget -q -O- http://php-apache; done\n\n\n\n\n\nNow open a new window of the same machine.\n\n\nAnd check the status of the hpa\n\n\nkubectl get hpa\n\n\n\n\nSample Output:\n\n\nkubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache/scale   50%       305%      1         10        3m\n\n\n\n\nNow if you check the pods it will be automatically scaled to the desired value.\n\n\nkubectl get pods\n\n\n\n\nSample Output\n\n\nkubectl get pods\nNAME                              READY     STATUS    RESTARTS   AGE\nload-generator-1930141919-1pqn0   1/1       Running   0          1h\nphp-apache-3815965786-2jmm9       1/1       Running   0          1h\nphp-apache-3815965786-4f0ck       1/1       Running   0          1h\nphp-apache-3815965786-73w24       1/1       Running   0          1h\nphp-apache-3815965786-80n2x       1/1       Running   0          1h\nphp-apache-3815965786-c6w0k       1/1       Running   0          1h\nphp-apache-3815965786-f06dg       1/1       Running   0          1h\nphp-apache-3815965786-nfs8d       1/1       Running   0          1h\nphp-apache-3815965786-phrhs       1/1       Running   0          1h\nphp-apache-3815965786-z6rnm       1/1       Running   0          1h\n\n\n\n\nStop load\n\n\nIn the terminal where we created the container with busybox image, terminate the load generation by typing \n + C\n\n\nThen we will verify the result state (after a minute or so)\n\n\nkubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache/scale   50%       0%        1         10        11m\n\n$ kubectl get deployment php-apache\nNAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nphp-apache   1         1         1            1           27m", 
            "title": "Auto Scaling Capacity with HPA"
        }, 
        {
            "location": "/6. Kubernetes Autoscaling/#kubernetes-horizonntal-pod-autoscaling", 
            "text": "With Horizontal Pod Autoscaling, Kubernetes automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with alpha support, on some other, application-provided metrics).  The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user", 
            "title": "Kubernetes Horizonntal Pod Autoscaling"
        }, 
        {
            "location": "/6. Kubernetes Autoscaling/#prerequisites", 
            "text": "Heapster monitoring needs to be deployed in the cluster as Horizontal Pod Autoscaler uses it to collect metrics.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/6. Kubernetes Autoscaling/#deploying-heapster", 
            "text": "Go to the below directory and create the deployment and services.  git clone https://github.com/kubernetes/heapster.git\ncd heapster\nkubectl apply -f deploy/kube-config/influxdb/\nkubectl apply -f deploy/kube-config/rbac/heapster-rbac.yaml  Validate that heapster, influxdb and grafana are started  kubectl get pods -n kube-system\nkubectl get svc -n kube-system  Now this will deploy the heapster monitoring.", 
            "title": "Deploying Heapster"
        }, 
        {
            "location": "/6. Kubernetes Autoscaling/#run-expose-php-apache-server", 
            "text": "To demonstrate Horizontal Pod Autoscaler we will use a custom docker image based on the php-apache image  kubectl run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80    Sample Output  kubectl run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80\nservice  php-apache  created\ndeployment  php-apache  created  To verify the created pod:  kubectl get pods  Wait untill the pod changes to running state.", 
            "title": "Run &amp; expose php-apache server"
        }, 
        {
            "location": "/6. Kubernetes Autoscaling/#create-horizontal-pod-autoscaler", 
            "text": "Now that the server is running, we will create the autoscaler using kubectl autoscale. The following command will create a Horizontal Pod Autoscaler that maintains between 1 and 10 replicas of the Pods controlled by the php-apache deployment we created in the first step of these instructions.  kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10  Sample Output  kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10\ndeployment  php-apache  autoscaled  We may check the current status of autoscaler by running:  kubectl get hpa  Sample Output:  kubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache   50%       0%        1         10        18s", 
            "title": "Create Horizontal Pod Autoscaler"
        }, 
        {
            "location": "/6. Kubernetes Autoscaling/#increase-load", 
            "text": "Now we can increase the load and trying testing what will happen.\nWe will start a container, and send an infinite loop of queries to the php-apache service  kubectl run -i --tty -n dev load-generator --image=busybox /bin/sh\n\nHit enter for command prompt\n\nwhile true; do wget -q -O- http://php-apache; done  Now open a new window of the same machine.  And check the status of the hpa  kubectl get hpa  Sample Output:  kubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache/scale   50%       305%      1         10        3m  Now if you check the pods it will be automatically scaled to the desired value.  kubectl get pods  Sample Output  kubectl get pods\nNAME                              READY     STATUS    RESTARTS   AGE\nload-generator-1930141919-1pqn0   1/1       Running   0          1h\nphp-apache-3815965786-2jmm9       1/1       Running   0          1h\nphp-apache-3815965786-4f0ck       1/1       Running   0          1h\nphp-apache-3815965786-73w24       1/1       Running   0          1h\nphp-apache-3815965786-80n2x       1/1       Running   0          1h\nphp-apache-3815965786-c6w0k       1/1       Running   0          1h\nphp-apache-3815965786-f06dg       1/1       Running   0          1h\nphp-apache-3815965786-nfs8d       1/1       Running   0          1h\nphp-apache-3815965786-phrhs       1/1       Running   0          1h\nphp-apache-3815965786-z6rnm       1/1       Running   0          1h", 
            "title": "Increase load"
        }, 
        {
            "location": "/6. Kubernetes Autoscaling/#stop-load", 
            "text": "In the terminal where we created the container with busybox image, terminate the load generation by typing   + C  Then we will verify the result state (after a minute or so)  kubectl get hpa\nNAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE\nphp-apache   Deployment/php-apache/scale   50%       0%        1         10        11m\n\n$ kubectl get deployment php-apache\nNAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nphp-apache   1         1         1            1           27m", 
            "title": "Stop load"
        }, 
        {
            "location": "/6. deploying_sample_app/", 
            "text": "Mini Project: Deploying Multi Tier Application Stack\n\n\nIn this project , you would write definitions for deploying the vote application stack with all compoments/tiers which include,\n\n\n\n\nvote ui\n\n\nredis\n\n\nworker\n\n\ndb\n\n\nresults ui\n\n\n\n\nTasks\n\n\n\n\nCreate deployments for all applications\n\n\nDefine services for each tier\n\n\nLaunch/appy the definitions\n\n\n\n\nFollowing table depicts the state of readiness of the above services.\n\n\n\n\n\n\n\n\nApp\n\n\nDeployment\n\n\nService\n\n\n\n\n\n\n\n\n\n\nvote\n\n\nready\n\n\nready\n\n\n\n\n\n\nredis\n\n\nin progress\n\n\nready\n\n\n\n\n\n\nworker\n\n\nin progress\n\n\nin progress\n\n\n\n\n\n\ndb\n\n\ntodo\n\n\ntodo\n\n\n\n\n\n\nresults\n\n\ntodo\n\n\ntodo\n\n\n\n\n\n\n\n\nDeploying the sample application\n\n\nTo create deploy the sample applications,\n\n\nkubectl create -f apps/voting/dev\n\n\n\n\nSample output is like:\n\n\nkubectl create -f voting-app.yaml\ndeployment \ndb\n created\nservice \ndb\n created\ndeployment \nredis\n created\nservice \nredis\n created\ndeployment \nvote\n created\nservice \nvote\n created\ndeployment \nworker\n created\ndeployment \nresults\n created\nservice \nresults\n created\n\n\n\n\nTo Validatecheck it:\n\n\nkubectl get svc -n dev\n\n\n\n\nSample Output is:\n\n\nkubectl get service voting-app\nNAME         CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nvote   10.97.104.243   \npending\n     80:31808/TCP   1h\n\n\n\n\nHere the port assigned is 31808, go to the browser and enter\n\n\nmasterip:31808\n\n\n\n\n\n\nThis will load the page where you can vote.\n\n\nTo check the result:\n\n\nkubectl get service result\n\n\n\n\nSample Output is:\n\n\nkubectl get service result\nNAME      CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nresult    10.101.112.16   \npending\n     80:32511/TCP   1h\n\n\n\n\nHere the port assigned is 32511, go to the browser and enter\n\n\nmasterip:32511\n\n\n\n\n\n\nThis is the page where we can see the results of the vote.", 
            "title": "Mini Project"
        }, 
        {
            "location": "/6. deploying_sample_app/#mini-project-deploying-multi-tier-application-stack", 
            "text": "In this project , you would write definitions for deploying the vote application stack with all compoments/tiers which include,   vote ui  redis  worker  db  results ui", 
            "title": "Mini Project: Deploying Multi Tier Application Stack"
        }, 
        {
            "location": "/6. deploying_sample_app/#tasks", 
            "text": "Create deployments for all applications  Define services for each tier  Launch/appy the definitions   Following table depicts the state of readiness of the above services.     App  Deployment  Service      vote  ready  ready    redis  in progress  ready    worker  in progress  in progress    db  todo  todo    results  todo  todo", 
            "title": "Tasks"
        }, 
        {
            "location": "/6. deploying_sample_app/#deploying-the-sample-application", 
            "text": "To create deploy the sample applications,  kubectl create -f apps/voting/dev  Sample output is like:  kubectl create -f voting-app.yaml\ndeployment  db  created\nservice  db  created\ndeployment  redis  created\nservice  redis  created\ndeployment  vote  created\nservice  vote  created\ndeployment  worker  created\ndeployment  results  created\nservice  results  created", 
            "title": "Deploying the sample application"
        }, 
        {
            "location": "/6. deploying_sample_app/#to-validatecheck-it", 
            "text": "kubectl get svc -n dev  Sample Output is:  kubectl get service voting-app\nNAME         CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nvote   10.97.104.243    pending      80:31808/TCP   1h  Here the port assigned is 31808, go to the browser and enter  masterip:31808   This will load the page where you can vote.  To check the result:  kubectl get service result  Sample Output is:  kubectl get service result\nNAME      CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nresult    10.101.112.16    pending      80:32511/TCP   1h  Here the port assigned is 32511, go to the browser and enter  masterip:32511   This is the page where we can see the results of the vote.", 
            "title": "To Validatecheck it:"
        }
    ]
}